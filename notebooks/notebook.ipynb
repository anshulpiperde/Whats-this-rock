{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Notes\n",
        "\n",
        "- improve using this [colab](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/keras/Keras_pipeline_with_Weights_and_Biases.ipynb#scrollTo=-303Shsata7Z)"
      ],
      "metadata": {
        "id": "JHpZwQ6o3r1q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run this and restart runtime"
      ],
      "metadata": {
        "id": "TE2vptAzU8Wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2"
      ],
      "metadata": {
        "id": "KeJ_9I925j-p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16f8b61b-d3fe-4d90-af8b-678d3a714817"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libcudnn8 is already the newest version (8.1.0.77-1+cuda11.2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 47 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip uninstall tensorflow -y\n",
        "pip install tf-nightly==2.10.0.dev20220427\n",
        "\n",
        "pip uninstall keras -y\n",
        "pip install keras-nightly==2.10.0.dev2022042807"
      ],
      "metadata": {
        "id": "eG8Qu95C4Sun",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16cde40b-7502-49f8-cdbe-5166dce20dc3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tf-nightly==2.10.0.dev20220427 in /usr/local/lib/python3.7/dist-packages (2.10.0.dev20220427)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427) (57.4.0)\n",
            "Requirement already satisfied: tb-nightly~=2.9.0.a in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427) (2.9.0a20220502)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427) (1.1.0)\n",
            "Requirement already satisfied: keras-nightly~=2.10.0.dev in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427) (2.10.0.dev2022042807)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427) (1.47.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427) (1.12)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427) (1.14.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427) (4.1.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427) (1.6.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427) (0.4.0)\n",
            "Requirement already satisfied: tf-estimator-nightly~=2.10.0.dev in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427) (2.10.0.dev2022071508)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427) (21.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427) (0.26.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427) (1.21.6)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427) (1.15.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427) (14.0.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427) (3.17.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427) (3.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tf-nightly==2.10.0.dev20220427) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tf-nightly==2.10.0.dev20220427) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427) (3.3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427) (0.6.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427) (3.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tf-nightly==2.10.0.dev20220427) (3.0.9)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras-nightly==2.10.0.dev2022042807 in /usr/local/lib/python3.7/dist-packages (2.10.0.dev2022042807)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Skipping tensorflow as it is not installed.\n",
            "WARNING: Skipping keras as it is not installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtV5lSDSBDAt",
        "outputId": "b914f043-4202-4185-ec0b-306ee8fdc48e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Jul 15 22:08:18 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P0    30W /  70W |  14628MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf Whats-this-rock/\n",
        "!git clone https://github.com/udaylunawat/Whats-this-rock.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3CTmNjsIAIl",
        "outputId": "b0277867-6bc8-4891-9822-96f6f803cf70"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Whats-this-rock'...\n",
            "remote: Enumerating objects: 515, done.\u001b[K\n",
            "remote: Counting objects: 100% (69/69), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 515 (delta 33), reused 46 (delta 18), pack-reused 446\u001b[K\n",
            "Receiving objects: 100% (515/515), 626.38 KiB | 15.66 MiB/s, done.\n",
            "Resolving deltas: 100% (300/300), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Whats-this-rock/\n",
        "!pip install -r requirements-dev.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wU2QkCalPvGr",
        "outputId": "34c37bba-ee74-49ff-fdbd-80d1dd3509a8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Whats-this-rock\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 1)) (1.0.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 2)) (4.1.2.30)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 3)) (1.5.12)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 4)) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 5)) (1.21.6)\n",
            "Requirement already satisfied: tf-nightly in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 6)) (2.10.0.dev20220427)\n",
            "Requirement already satisfied: keras-nightly in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 7)) (2.10.0.dev2022042807)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 8)) (0.17.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 9)) (0.12.21)\n",
            "Requirement already satisfied: keras-cv in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 10)) (0.2.8)\n",
            "Requirement already satisfied: split-folders in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 11)) (0.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements-dev.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements-dev.txt (line 1)) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements-dev.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle->-r requirements-dev.txt (line 3)) (6.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle->-r requirements-dev.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle->-r requirements-dev.txt (line 3)) (4.64.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle->-r requirements-dev.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle->-r requirements-dev.txt (line 3)) (2022.6.15)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle->-r requirements-dev.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle->-r requirements-dev.txt (line 3)) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements-dev.txt (line 4)) (2022.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->-r requirements-dev.txt (line 6)) (3.17.3)\n",
            "Requirement already satisfied: tb-nightly~=2.9.0.a in /usr/local/lib/python3.7/dist-packages (from tf-nightly->-r requirements-dev.txt (line 6)) (2.9.0a20220502)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->-r requirements-dev.txt (line 6)) (1.12)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->-r requirements-dev.txt (line 6)) (0.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->-r requirements-dev.txt (line 6)) (4.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tf-nightly->-r requirements-dev.txt (line 6)) (57.4.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->-r requirements-dev.txt (line 6)) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->-r requirements-dev.txt (line 6)) (3.3.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->-r requirements-dev.txt (line 6)) (1.6.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->-r requirements-dev.txt (line 6)) (0.26.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->-r requirements-dev.txt (line 6)) (3.1.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->-r requirements-dev.txt (line 6)) (14.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->-r requirements-dev.txt (line 6)) (1.47.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->-r requirements-dev.txt (line 6)) (1.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tf-nightly->-r requirements-dev.txt (line 6)) (21.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->-r requirements-dev.txt (line 6)) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->-r requirements-dev.txt (line 6)) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly->-r requirements-dev.txt (line 6)) (1.1.0)\n",
            "Requirement already satisfied: tf-estimator-nightly~=2.10.0.dev in /usr/local/lib/python3.7/dist-packages (from tf-nightly->-r requirements-dev.txt (line 6)) (2.10.0.dev2022071508)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tf-nightly->-r requirements-dev.txt (line 6)) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tf-nightly->-r requirements-dev.txt (line 6)) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.9.0.a->tf-nightly->-r requirements-dev.txt (line 6)) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.9.0.a->tf-nightly->-r requirements-dev.txt (line 6)) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.9.0.a->tf-nightly->-r requirements-dev.txt (line 6)) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.9.0.a->tf-nightly->-r requirements-dev.txt (line 6)) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.9.0.a->tf-nightly->-r requirements-dev.txt (line 6)) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.9.0.a->tf-nightly->-r requirements-dev.txt (line 6)) (3.3.7)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.9.0.a->tf-nightly->-r requirements-dev.txt (line 6)) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.9.0.a->tf-nightly->-r requirements-dev.txt (line 6)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.9.0.a->tf-nightly->-r requirements-dev.txt (line 6)) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.9.0.a->tf-nightly->-r requirements-dev.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tb-nightly~=2.9.0.a->tf-nightly->-r requirements-dev.txt (line 6)) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tb-nightly~=2.9.0.a->tf-nightly->-r requirements-dev.txt (line 6)) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tb-nightly~=2.9.0.a->tf-nightly->-r requirements-dev.txt (line 6)) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->-r requirements-dev.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->kaggle->-r requirements-dev.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.9.0.a->tf-nightly->-r requirements-dev.txt (line 6)) (3.2.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->-r requirements-dev.txt (line 8)) (2.7.1)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements-dev.txt (line 9)) (0.1.2)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements-dev.txt (line 9)) (3.1.27)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements-dev.txt (line 9)) (0.4.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements-dev.txt (line 9)) (1.7.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements-dev.txt (line 9)) (2.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements-dev.txt (line 9)) (5.4.8)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements-dev.txt (line 9)) (1.0.9)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements-dev.txt (line 9)) (3.13)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements-dev.txt (line 9)) (7.1.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements-dev.txt (line 9)) (1.2.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb->-r requirements-dev.txt (line 9)) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb->-r requirements-dev.txt (line 9)) (5.0.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tf-nightly->-r requirements-dev.txt (line 6)) (3.0.9)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle->-r requirements-dev.txt (line 3)) (1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import keras_cv\n",
        "except:\n",
        "    print(\"Install Tensorflow >= 2.9 and Restart runtime.\")"
      ],
      "metadata": {
        "id": "a4NbVJC5wc8m"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUSuNjlQZSi-"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj8_Qvwne8e7"
      },
      "source": [
        "### Uploading Kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Whats-this-rock/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TATjQ6IsIgjj",
        "outputId": "62f4ba45-7451-4100-bc1d-a7bcfca3d191"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Whats-this-rock\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "Xdg9VTNbFeCc",
        "outputId": "78c34211-d3b2-443e-f309-2e1b7cef44ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload Kaggle.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-89510e53-c12c-42ba-9fd9-21a812ec5969\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-89510e53-c12c-42ba-9fd9-21a812ec5969\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (1).json\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "print(\"Upload Kaggle.json\")\n",
        "files.upload();"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sh setup.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRb3HeTCXyOW",
        "outputId": "ae21a940-adfe-41de-f44d-7b33a37d4095"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading igneous-metamorphic-sedimentary-rocks-and-minerals.zip to data/0_raw\n",
            " 93% 324M/349M [00:02<00:00, 135MB/s]\n",
            "100% 349M/349M [00:02<00:00, 132MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rEMinI4fChL"
      },
      "outputs": [],
      "source": [
        "!python preprocess.py --root data/1_extracted/Rock_Dataset/ \\\n",
        "                      --remove_class minerals\n",
        "                    #   --undersample \\\n",
        "# data/1_extracted/Rock_Dataset/sedimentary\\ rocks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Undersampling using pandas"
      ],
      "metadata": {
        "id": "_jWyOV9DhZKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import os\n",
        "# balanced_df = pd.read_csv(os.path.join('data/3_consume/', 'balanced_image_paths.csv'), index_col = 0)\n",
        "# balanced_df['classes'].value_counts()"
      ],
      "metadata": {
        "id": "UXRCs-nOrxx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# all_df = pd.read_csv(os.path.join('data/3_consume/', 'all_image_paths.csv'), index_col = 0)\n",
        "# all_df['classes'].value_counts()"
      ],
      "metadata": {
        "id": "Oz-OGOQhUCAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # https://stackoverflow.com/a/44546263/9292995\n",
        "# drop_df = pd.merge(all_df, balanced_df, on=['image_path','classes'], how='outer', indicator=True).query(\"_merge != 'both'\").drop('_merge', axis=1).reset_index(drop=True)\n",
        "# drop_df['image_path'].apply(lambda x:os.remove(x))"
      ],
      "metadata": {
        "id": "pEyPK3DWUrfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# balanced_df"
      ],
      "metadata": {
        "id": "iyW_mLBgHXQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run pipeline in notebook"
      ],
      "metadata": {
        "id": "bE0IrPrIW-D1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "# class dotdict(dict):\n",
        "#     \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
        "#     __getattr__ = dict.get\n",
        "#     __setattr__ = dict.__setitem__\n",
        "#     __delattr__ = dict.__delitem__\n",
        "\n",
        "\n",
        "# with open('config.json') as f:\n",
        "#     config = dotdict(json.load(f))"
      ],
      "metadata": {
        "id": "cXZhSwXMXCI9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = dict(\n",
        "    root_dir = \"data/4_tfds_dataset\",\n",
        "    project_name = \"Whats-this-rock\",\n",
        "    model_name = \"resnet50\",\n",
        "    sample_size = 1.0,\n",
        "    augment= True,\n",
        "    optimizer= \"Adam\",\n",
        "    learning_rate = 0.0005,\n",
        "    batch_size = 64,\n",
        "    epochs = 5,\n",
        "    image_size = 224,\n",
        "    trainable = False,\n",
        "    lr_decay_rate = 0.7,\n",
        "    loss_fn = \"CategoricalCrossentropy(label_smoothing=0.1)\",\n",
        "    metrics = ['acc'],\n",
        "    earlystopping_patience = 5,\n",
        "    notes= \"keras-cv augment run\"\n",
        ")"
      ],
      "metadata": {
        "id": "Swg2VMz_DcTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgXIxDWLXCGF",
        "outputId": "b60feaec-8da5-43e6-c58a-41f39b2c211d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'root_dir': 'data/1_extracted/Rock_Dataset/', 'project_name': 'rock_classification', 'model_name': 'efficientnet', 'sample_size': 1.0, 'augment': 'False', 'optimizer': 'Adam', 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 20, 'image_size': 224, 'trainable': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from data_utilities import get_data\n",
        "# from model_utilities import get_generators\n",
        "\n",
        "# train_df, test_df = get_data(config.sample_size)\n",
        "\n",
        "# train_generator, val_generator, test_generator = get_generators(\n",
        "#     config, train_df, test_df)\n",
        "\n",
        "# num_classes = len(train_generator.class_indices)"
      ],
      "metadata": {
        "id": "9jy3nQDEXCC9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using keras_cv"
      ],
      "metadata": {
        "id": "wmIeqgzvHAQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import applications\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import optimizers\n",
        "import keras_cv\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "metadata": {
        "id": "OZYbKL20HQS0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf data/4_tfds_dataset\n",
        "!mkdir data/4_tfds_dataset checkpoints\n",
        "\n",
        "# https://stackoverflow.com/a/64006242/9292995\n",
        "import splitfolders\n",
        "\n",
        "# If your datasets is balanced (each class has the same number of samples), choose ratio otherwise fixed.\n",
        "splitfolders.fixed('data/2_processed', output=\"data/4_tfds_dataset\", oversample=True,\n",
        "                   seed=1337) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5PGRYw4qEmN",
        "outputId": "bcf0a4f5-5527-406f-98b8-6febb37fcf68"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying files: 4537 files [00:00, 7701.75 files/s]\n",
            "Oversampling: 3 classes [00:00,  7.50 classes/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.tensorflow.org/datasets/api_docs/python/tfds/folder_dataset/ImageFolder\n",
        "builder = tfds.ImageFolder('data/4_tfds_dataset')\n",
        "print(builder.info)  # num examples, labels... are automatically calculated\n",
        "data = builder.as_dataset(split=None, as_supervised=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRHkxw-GrA0D",
        "outputId": "529a098b-8d82-4c6f-efd5-4d0313dbc280"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfds.core.DatasetInfo(\n",
            "    name='image_folder',\n",
            "    version=1.0.0,\n",
            "    description='Generic image classification dataset.',\n",
            "    homepage='https://www.tensorflow.org/datasets/catalog/image_folder',\n",
            "    features=FeaturesDict({\n",
            "        'image': Image(shape=(None, None, 3), dtype=tf.uint8),\n",
            "        'image/filename': Text(shape=(), dtype=tf.string),\n",
            "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=3),\n",
            "    }),\n",
            "    total_num_examples=6741,\n",
            "    splits={\n",
            "        'test': 300,\n",
            "        'train': 6141,\n",
            "        'val': 300,\n",
            "    },\n",
            "    supervised_keys=('image', 'label'),\n",
            "    citation=\"\"\"\"\"\",\n",
            "    redistribution_info=,\n",
            ")\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = builder.info.features['label'].num_classes\n",
        "config[\"num_classes\"] = num_classes"
      ],
      "metadata": {
        "id": "uNImMTZRzW4V"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # OPTIONAL show dataset\n",
        "# ds = builder.as_dataset(split='train', shuffle_files=True)\n",
        "# tfds.show_examples(ds, builder.info)"
      ],
      "metadata": {
        "id": "5-B8jUCPr2vp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # https://stackoverflow.com/a/37343690/9292995\n",
        "# # https://keras.io/guides/keras_cv/cut_mix_mix_up_and_rand_augment/\n",
        "\n",
        "IMAGE_SIZE = (config[\"image_size\"], config[\"image_size\"])\n",
        "\n",
        "\n",
        "def to_dict(image, label):\n",
        "    image = tf.image.resize(image, IMAGE_SIZE)\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    label = tf.one_hot(label, config[\"num_classes\"])\n",
        "    return {\"images\": image, \"labels\": label}\n",
        "\n",
        "\n",
        "def prepare_dataset(dataset, split):\n",
        "    if split == \"train\":\n",
        "        return (\n",
        "            dataset.shuffle(10 * config[\"batch_size\"])\n",
        "            .map(to_dict, num_parallel_calls=AUTOTUNE)\n",
        "            .batch(config[\"batch_size\"])\n",
        "        )\n",
        "    elif split == \"val\" or split == \"test\":\n",
        "        return (\n",
        "            dataset.map(to_dict, num_parallel_calls=AUTOTUNE)\n",
        "            .batch(config[\"batch_size\"])\n",
        "        )\n",
        "\n",
        "\n",
        "def load_dataset(split=\"train\"):\n",
        "    dataset = data[split]\n",
        "    return prepare_dataset(dataset, split)\n",
        "\n",
        "\n",
        "train_dataset = load_dataset()"
      ],
      "metadata": {
        "id": "TEOESg7jsCWm"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_dataset(dataset, title):\n",
        "    plt.figure(figsize=(12, 12)).suptitle(title, fontsize=18)\n",
        "    for i, samples in enumerate(iter(dataset.take(9))):\n",
        "        images = samples[\"images\"]\n",
        "        plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(images[0].numpy().astype(\"uint8\"))\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()\n",
        "\n",
        "def apply_rand_augment(inputs):\n",
        "    inputs[\"images\"] = rand_augment(inputs[\"images\"])\n",
        "    return inputs\n",
        "\n",
        "rand_augment = keras_cv.layers.RandAugment(\n",
        "    value_range=(0, 255),\n",
        "    augmentations_per_image=3,\n",
        "    magnitude=0.3,\n",
        "    magnitude_stddev=0.2,\n",
        "    rate=0.5,\n",
        ")\n",
        "\n",
        "cut_mix = keras_cv.layers.CutMix()\n",
        "mix_up = keras_cv.layers.MixUp()\n",
        "\n",
        "\n",
        "def cut_mix_and_mix_up(samples):\n",
        "    samples = cut_mix(samples, training=True)\n",
        "    samples = mix_up(samples, training=True)\n",
        "    return samples\n",
        "\n",
        "def preprocess_for_model(inputs):\n",
        "    images, labels = inputs[\"images\"], inputs[\"labels\"]\n",
        "    images = tf.cast(images, tf.float32)\n",
        "    return images, labels"
      ],
      "metadata": {
        "id": "n9N0drY5y4BH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize_dataset(train_dataset, title=\"Before Augmentation\")"
      ],
      "metadata": {
        "id": "yl7A5NfqjgdL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataset = load_dataset().map(apply_rand_augment, num_parallel_calls=AUTOTUNE)\n",
        "# visualize_dataset(train_dataset, title=\"After RandAugment\")"
      ],
      "metadata": {
        "id": "gaxjSvq_Sill"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataset = load_dataset().map(cut_mix_and_mix_up, num_parallel_calls=AUTOTUNE)\n",
        "# visualize_dataset(train_dataset, title=\"After cut_mix and mix_up\")"
      ],
      "metadata": {
        "id": "ujt9zjXTZsRi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = (\n",
        "    load_dataset()\n",
        "    .map(apply_rand_augment, num_parallel_calls=AUTOTUNE)\n",
        "    .map(cut_mix_and_mix_up, num_parallel_calls=AUTOTUNE)\n",
        ")\n",
        "\n",
        "# visualize_dataset(train_dataset, \"CutMix, MixUp and RandAugment\")\n",
        "\n",
        "train_dataset = train_dataset.map(preprocess_for_model, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "val_dataset = load_dataset(split=\"val\")\n",
        "val_dataset = val_dataset.map(preprocess_for_model, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "test_dataset = load_dataset(split=\"test\")\n",
        "test_dataset = test_dataset.map(preprocess_for_model, num_parallel_calls=AUTOTUNE)"
      ],
      "metadata": {
        "id": "HhV0wZaHPZ5-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (config[\"image_size\"], config[\"image_size\"]) + (3,)\n",
        "\n",
        "def get_model():\n",
        "    input_t = keras.Input(shape=(config[\"image_size\"], config[\"image_size\"], 3))\n",
        "    res_model = keras.applications.ResNet50(include_top=False,\n",
        "                                            weights=\"imagenet\",\n",
        "                                            input_tensor=input_t)\n",
        "\n",
        "    to_res = IMAGE_SIZE\n",
        "\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.Lambda(lambda image: tf.image.resize(image, to_res)))\n",
        "    model.add(res_model)\n",
        "    model.add(keras.layers.Flatten())\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "    model.add(keras.layers.Dense(256, activation='relu'))\n",
        "    model.add(keras.layers.Dropout(0.5))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "    model.add(keras.layers.Dense(128, activation='relu'))\n",
        "    model.add(keras.layers.Dropout(0.5))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "    model.add(keras.layers.Dense(64, activation='relu'))\n",
        "    model.add(keras.layers.Dropout(0.5))\n",
        "    model.add(keras.layers.BatchNormalization())\n",
        "    model.add(keras.layers.Dense(num_classes, activation='softmax'))\n",
        "\n",
        "    # Notice that we use label_smoothing=0.1 in the loss function. \n",
        "    # When using MixUp, label smoothing is highly recommended.\n",
        "    model.compile(\n",
        "        loss=losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
        "        optimizer=optimizers.Adam(config[\"learning_rate\"]),\n",
        "        metrics=config[\"metrics\"],\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "6MGIEx0xPyOV"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# def configure_for_performance(ds):\n",
        "#   ds = ds.cache()\n",
        "#   ds = ds.shuffle(buffer_size=500)\n",
        "#   ds = ds.batch(config.batch_size)\n",
        "#   ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
        "#   return ds\n",
        "\n",
        "# train_dataset = configure_for_performance(train_dataset)\n",
        "# val_dataset = configure_for_performance(val_dataset)"
      ],
      "metadata": {
        "id": "UWz43CSY65cZ"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config['epochs'] = 100\n",
        "config['dry_run'] = True\n",
        "config['learning_rate'] = \n",
        "os.environ['WANDB_MODE'] = 'run'"
      ],
      "metadata": {
        "id": "q9ZCYNrIAYbv"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "\"\"\"\n",
        "Trains a model on the dataset.\n",
        "Designed to show how to do a simple wandb integration with keras.\n",
        "\"\"\"\n",
        "from data_utilities import get_data\n",
        "from model_utilities import get_generators, get_optimizer, get_model_weights\n",
        "from models import finetune\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow.random import set_seed\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.models import load_model\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "from wandb.keras import WandbCallback\n",
        "import wandb\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "import argparse\n",
        "import sys\n",
        "import os\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# *IMPORANT*: Have to do this line *before* importing tensorflow\n",
        "# os.environ['PYTHONHASHSEED'] = str(1)\n",
        "\n",
        "\n",
        "def reset_random_seeds():\n",
        "    os.environ['PYTHONHASHSEED'] = str(1)\n",
        "    set_seed(1)\n",
        "    np.random.seed(1)\n",
        "    random.seed(1)\n",
        "\n",
        "\n",
        "reset_random_seeds()\n",
        "args = config\n",
        "\n",
        "if args[\"dry_run\"]:\n",
        "    wandb.mode = \"offline\"\n",
        "else:\n",
        "    wandb.mode = \"online\"\n",
        "\n",
        "run = wandb.init(\n",
        "    project=args[\"project_name\"],\n",
        "    notes=args[\"notes\"],\n",
        "    allow_val_change=True)\n",
        "\n",
        "wandb.config.update(config)\n",
        "\n",
        "print(f'Augmentation - {config[\"augment\"]}\\nmodel name - {config[\"model_name\"]}\\nconfig - {config}')\n",
        "\n",
        "# build model\n",
        "\n",
        "wandb.log({\"num_classes\": num_classes})\n",
        "labels = builder.info.features['label'].names\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "model = get_model()\n",
        "\n",
        "# model.summary()\n",
        "\n",
        "# model_checkpoint = ModelCheckpoint(\"checkpoints/\"+\n",
        "#                                    f\"{wandb.run.name}-\"+config[\"model_name\"]+\n",
        "#                                    \"-epoch-{epoch}_val_accuracy-{val_acc:.2f}.hdf5\", save_best_only=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.8, patience=5, verbose=1)\n",
        "earlystopper = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', patience=config['earlystopping_patience'], verbose=1, mode='auto',\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "wandbcallback = WandbCallback(training_data=train_dataset, \n",
        "                              validation_data=val_dataset,\n",
        "                              labels=labels,\n",
        "                              save_model=True,\n",
        "                              monitor='val_loss',\n",
        "                              log_weights=True,\n",
        "                              log_evaluation=True,\n",
        "                              validation_steps=5)\n",
        "\n",
        "# Define WandbCallback for experiment tracking\n",
        "\n",
        "callbacks = [earlystopper, reduce_lr, wandbcallback]\n",
        "model = get_model()\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=config[\"epochs\"],\n",
        "    validation_data=val_dataset,\n",
        "    callbacks = callbacks\n",
        ")\n",
        "# trained_model_artifact = wandb.Artifact(config[\"model_name\"], type = \"model\", description=config[\"notes\"])\n",
        "# trained_model_artifact.add_dir('checkpoints')\n",
        "# run.log_artifact(trained_model_artifact)\n",
        "\n",
        "run.finish()\n",
        "\n",
        "\n",
        "# strategy = tf.distribute.MirroredStrategy()\n",
        "# with strategy.scope():\n",
        "#     model = get_model()\n",
        "#     history = model.fit(\n",
        "#         train_dataset,\n",
        "#         epochs=3,\n",
        "#         validation_data=val_dataset,\n",
        "#         callbacks = callbacks\n",
        "#     )\n",
        "\n",
        "\n",
        "# Confution Matrix and Classification Report\n",
        "# Y_pred = model.predict(\n",
        "#     val_generator,\n",
        "#     len(val_generator) // config[\"batch_size\"] + 1)\n",
        "# y_pred = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "# print('Confusion Matrix with Validation data')\n",
        "# print(confusion_matrix(val_generator.classes, y_pred))\n",
        "# print('Classification Report')\n",
        "# cl_report = classification_report(\n",
        "#     val_generator.classes,\n",
        "#     y_pred,\n",
        "#     target_names=labels,\n",
        "#     output_dict=True)\n",
        "# # print(pd.DataFrame(cl_report))\n",
        "# print(cl_report)\n",
        "# run.finish()"
      ],
      "metadata": {
        "id": "Q4iqtHHtoBRf"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "pXf3ueNbB7PD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile config.json\n",
        "\n",
        "{\n",
        "    \"root_dir\" : \"data/1_extracted/Rock_Dataset/\",\n",
        "    \"project_name\" : \"rock_classification\",\n",
        "    \"model_name\" : \"efficientnet\",\n",
        "    \"sample_size\" : 0.5,\n",
        "    \"augment\": \"False\",\n",
        "    \"optimizer\": \"Adam\",\n",
        "    \"learning_rate\" : 0.005,\n",
        "    \"batch_size\" : 128,\n",
        "    \"epochs\" : 50,\n",
        "    \"image_size\" : 224,\n",
        "    \"trainable\" : false\n",
        "}"
      ],
      "metadata": {
        "id": "HrgJfZD7sGq1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5edbe0f2-01fd-4b95-f962-da1bd6b06479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing config.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- use dry_run if you don't want to log to wandb"
      ],
      "metadata": {
        "id": "TGNLmkaSPFd1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6xHF-hb_j_t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "726c27bd-ac58-47c8-b9d6-d04f923d0e24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file 'train.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!python train.py --dry_run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa9mUak6P7bm"
      },
      "source": [
        "# Weights and Biases Sweeps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mt6c5b0rjC45",
        "outputId": "5f22e81b-4667-44cb-e005-29c53c5c8dec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ],
      "source": [
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = '3pw3l2bd'\n",
        "%cd /content/Whats-this-rock\n",
        "!wandb agent rock-classifiers/Whats-this-rock/$sweep_id"
      ],
      "metadata": {
        "id": "kAPZwDqbr2fj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07cd33db-ee59-4cc7-fbf2-e23dda7428f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Whats-this-rock\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Login to W&B to use the sweep agent feature\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "Aborted!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb sweep --update rock-classifiers/Whats-this-rock/$sweep_id sweep.yaml"
      ],
      "metadata": {
        "id": "bK9aivfKsavT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52181119-d88e-4362-c957-34b250782986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/wandb\", line 5, in <module>\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/__init__.py\", line 26, in <module>\n",
            "    from wandb import sdk as wandb_sdk\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/__init__.py\", line 7, in <module>\n",
            "    from .wandb_artifacts import Artifact  # noqa: F401\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/wandb_artifacts.py\", line 29, in <module>\n",
            "    from wandb.apis import InternalApi, PublicApi\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/apis/__init__.py\", line 31, in <module>\n",
            "    from .internal import Api as InternalApi  # noqa\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/apis/internal.py\", line 1, in <module>\n",
            "    from wandb.sdk.internal.internal_api import Api as InternalApi\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/internal/internal_api.py\", line 33, in <module>\n",
            "    from ..lib.git import GitRepo\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/wandb/sdk/lib/git.py\", line 191, in <module>\n",
            "    from git import Repo, exc  # type: ignore\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/git/__init__.py\", line 43, in <module>\n",
            "    from git.objects import *               # @NoMove @IgnorePep8\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/git/objects/__init__.py\", line 7, in <module>\n",
            "    from .base import *\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/git/objects/base.py\", line 13, in <module>\n",
            "    from .util import get_object_type_by_name\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/git/objects/util.py\", line 145, in <module>\n",
            "    class tzoffset(tzinfo):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/git/objects/util.py\", line 151, in tzoffset\n",
            "    def __reduce__(self) -> Tuple[Type['tzoffset'], Tuple[float, str]]:\n",
            "  File \"/usr/lib/python3.7/typing.py\", line 251, in inner\n",
            "    return cached(*args, **kwds)\n",
            "  File \"/usr/lib/python3.7/typing.py\", line 636, in __getitem__\n",
            "    return _subs_tvars(self, self.__parameters__, params)\n",
            "  File \"/usr/lib/python3.7/typing.py\", line 196, in _subs_tvars\n",
            "    return tp.copy_with(tuple(new_args))\n",
            "  File \"/usr/lib/python3.7/typing.py\", line 640, in copy_with\n",
            "    return _GenericAlias(self.__origin__, params, name=self._name, inst=self._inst)\n",
            "  File \"/usr/lib/python3.7/typing.py\", line 621, in __init__\n",
            "    self.__parameters__ = _collect_type_vars(params)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/typing_extensions.py\", line 148, in _collect_type_vars\n",
            "    return tuple(tvars)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jMcGudGAUoe"
      },
      "source": [
        "# Deploy Telegram Bot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaYiDe5Me-kc"
      },
      "source": [
        "Uploading secrets with telegram key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hflH5kVue6WT"
      },
      "outputs": [],
      "source": [
        "print(\"Upload secrets.json\")\n",
        "files.upload();"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Whats-this-rock.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}