{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rock classifier training",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeJ_9I925j-p",
        "outputId": "12dd9bb5-673a-4475-c2c3-5ed6ed5fe0d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following packages will be REMOVED:\n",
            "  libcudnn8-dev\n",
            "The following held packages will be changed:\n",
            "  libcudnn8\n",
            "The following packages will be upgraded:\n",
            "  libcudnn8\n",
            "1 upgraded, 0 newly installed, 1 to remove and 17 not upgraded.\n",
            "Need to get 430 MB of archives.\n",
            "After this operation, 3,139 MB disk space will be freed.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libcudnn8 8.1.0.77-1+cuda11.2 [430 MB]\n",
            "Fetched 430 MB in 9s (47.5 MB/s)\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 155680 files and directories currently installed.)\r\n",
            "Removing libcudnn8-dev (8.0.5.39-1+cuda11.1) ...\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 155658 files and directories currently installed.)\r\n",
            "Preparing to unpack .../libcudnn8_8.1.0.77-1+cuda11.2_amd64.deb ...\r\n",
            "Unpacking libcudnn8 (8.1.0.77-1+cuda11.2) over (8.0.5.39-1+cuda11.1) ...\r\n",
            "Setting up libcudnn8 (8.1.0.77-1+cuda11.2) ...\r\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n",
        "# pip uninstall tensorflow -y\n",
        "# pip install tf-nightly==2.10.0.dev20220427\n",
        "\n",
        "# pip uninstall keras -y\n",
        "# pip install keras-nightly==2.10.0.dev2022042807"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3CTmNjsIAIl",
        "outputId": "b184d741-6089-4197-926a-8b2f48a00347"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Whats-this-rock'...\n",
            "remote: Enumerating objects: 745, done.\u001b[K\n",
            "remote: Counting objects: 100% (299/299), done.\u001b[K\n",
            "remote: Compressing objects: 100% (204/204), done.\u001b[K\n",
            "remote: Total 745 (delta 183), reused 205 (delta 94), pack-reused 446\u001b[K\n",
            "Receiving objects: 100% (745/745), 3.03 MiB | 27.69 MiB/s, done.\n",
            "Resolving deltas: 100% (450/450), done.\n",
            "/content/Whats-this-rock\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 1)) (1.0.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 2)) (4.6.0.66)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 3)) (1.5.12)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 4)) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements-dev.txt (line 5)) (1.21.6)\n",
            "Collecting tf-nightly==2.10.0.dev20220427\n",
            "  Downloading tf_nightly-2.10.0.dev20220427-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (503.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 503.2 MB 4.6 kB/s \n",
            "\u001b[?25hCollecting keras-nightly==2.10.0.dev2022042807\n",
            "  Downloading keras_nightly-2.10.0.dev2022042807-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 53.8 MB/s \n",
            "\u001b[?25hCollecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.17.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 50.6 MB/s \n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.13.1-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 44.3 MB/s \n",
            "\u001b[?25hCollecting keras-cv\n",
            "  Downloading keras_cv-0.2.10-py3-none-any.whl (227 kB)\n",
            "\u001b[K     |████████████████████████████████| 227 kB 45.1 MB/s \n",
            "\u001b[?25hCollecting split-folders\n",
            "  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
            "Collecting tf-estimator-nightly~=2.10.0.dev\n",
            "  Downloading tf_estimator_nightly-2.10.0.dev2022072808-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[K     |████████████████████████████████| 438 kB 52.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (3.3.0)\n",
            "Collecting flatbuffers<2,>=1.12\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (3.1.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (1.47.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (0.26.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (3.17.3)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (14.0.6)\n",
            "Collecting gast<=0.4.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (57.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (4.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (21.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (1.14.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (1.15.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (1.6.3)\n",
            "Collecting tb-nightly~=2.9.0.a\n",
            "  Downloading tb_nightly-2.9.0a20220502-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 26.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly~=2.9.0.a->tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (3.2.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements-dev.txt (line 1)) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements-dev.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements-dev.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle->-r requirements-dev.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle->-r requirements-dev.txt (line 3)) (4.64.0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle->-r requirements-dev.txt (line 3)) (6.1.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements-dev.txt (line 4)) (2022.1)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->-r requirements-dev.txt (line 8)) (2.7.1)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 43.8 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements-dev.txt (line 9)) (5.4.8)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements-dev.txt (line 9)) (2.3)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 48.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements-dev.txt (line 9)) (7.1.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements-dev.txt (line 9)) (3.13)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 41.7 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 50.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tf-nightly==2.10.0.dev20220427->-r requirements-dev.txt (line 6)) (3.0.9)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle->-r requirements-dev.txt (line 3)) (1.3)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=64208291c69b35b55b42e294e0d69af300e1d35462434c75dd6f07f96b13e4aa\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, tf-estimator-nightly, tb-nightly, shortuuid, setproctitle, sentry-sdk, pathtools, keras-nightly, GitPython, gast, flatbuffers, docker-pycreds, wandb, tf-nightly, tensorflow-addons, split-folders, keras-cv\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 flatbuffers-1.12 gast-0.4.0 gitdb-4.0.9 keras-cv-0.2.10 keras-nightly-2.10.0.dev2022042807 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.1 shortuuid-1.0.9 smmap-5.0.0 split-folders-0.5.1 tb-nightly-2.9.0a20220502 tensorflow-addons-0.17.1 tf-estimator-nightly-2.10.0.dev2022072808 tf-nightly-2.10.0.dev20220427 wandb-0.13.1\n"
          ]
        }
      ],
      "source": [
        "!rm -rf Whats-this-rock/\n",
        "!git clone https://github.com/udaylunawat/Whats-this-rock.git\n",
        "%cd /content/Whats-this-rock/\n",
        "!pip install -r requirements-dev.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4NbVJC5wc8m"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import keras_cv\n",
        "except:\n",
        "    print(\"Install Tensorflow >= 2.9 and Restart runtime.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NonXdHZjSwBP"
      },
      "outputs": [],
      "source": [
        "# # Kill runtime\n",
        "# import os\n",
        "# os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUSuNjlQZSi-"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YhGXETTXICk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ded35f5d-28a6-443e-ef34-033aba798c3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wj8_Qvwne8e7"
      },
      "source": [
        "### Uploading Kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TATjQ6IsIgjj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e739b2ad-5f97-467b-c98e-19dc7adf64b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Whats-this-rock\n"
          ]
        }
      ],
      "source": [
        "%cd /content/Whats-this-rock/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1r8vclU4_2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4c0a90c-c653-4f25-8bb8-69e742926f06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-06 14:43:32--  https://www.dropbox.com/s/9mhxs1p4mgb8gyh/kaggle.json\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.1.18, 2620:100:6016:18::a27d:112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.1.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/9mhxs1p4mgb8gyh/kaggle.json [following]\n",
            "--2022-08-06 14:43:33--  https://www.dropbox.com/s/raw/9mhxs1p4mgb8gyh/kaggle.json\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uccbe5b0675cbbd8f7d49f574791.dl.dropboxusercontent.com/cd/0/inline/BqhBkVnK0h5BaaQyrmjD9HSB3xUC8b5T5KMcTI2S0pGPn9YzyWKbDNOZ_GldI2wmTsgqh3jK86ZXGmZS-fNlEAsVCHYYz9Rn0fehX7UuUzV5aqAFyrERsjL9KMKX527b7YqBvOXSVIlWjGV9toeYt2r2IRIamQ85FFJ_lraMyvUGSQ/file# [following]\n",
            "--2022-08-06 14:43:33--  https://uccbe5b0675cbbd8f7d49f574791.dl.dropboxusercontent.com/cd/0/inline/BqhBkVnK0h5BaaQyrmjD9HSB3xUC8b5T5KMcTI2S0pGPn9YzyWKbDNOZ_GldI2wmTsgqh3jK86ZXGmZS-fNlEAsVCHYYz9Rn0fehX7UuUzV5aqAFyrERsjL9KMKX527b7YqBvOXSVIlWjGV9toeYt2r2IRIamQ85FFJ_lraMyvUGSQ/file\n",
            "Resolving uccbe5b0675cbbd8f7d49f574791.dl.dropboxusercontent.com (uccbe5b0675cbbd8f7d49f574791.dl.dropboxusercontent.com)... 162.125.1.15, 2620:100:6016:15::a27d:10f\n",
            "Connecting to uccbe5b0675cbbd8f7d49f574791.dl.dropboxusercontent.com (uccbe5b0675cbbd8f7d49f574791.dl.dropboxusercontent.com)|162.125.1.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 70 [text/plain]\n",
            "Saving to: ‘kaggle.json’\n",
            "\n",
            "kaggle.json         100%[===================>]      70  --.-KB/s    in 0s      \n",
            "\n",
            "2022-08-06 14:43:33 (11.2 MB/s) - ‘kaggle.json’ saved [70/70]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import files\n",
        "# print(\"Upload Kaggle.json\")\n",
        "# files.upload();\n",
        "\n",
        "\n",
        "! wget https://www.dropbox.com/s/9mhxs1p4mgb8gyh/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXu-3mstXJ0k"
      },
      "source": [
        "# Load Data and Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhCpz6g_zMqG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07656858-44d9-406d-9e5c-8174315c43af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading rock-classification.zip to data/0_raw\n",
            "100% 159M/159M [00:00<00:00, 182MB/s]\n",
            "100% 159M/159M [00:00<00:00, 199MB/s]\n",
            "\n",
            "\n",
            "Removing corrupted images...\n",
            "Total images before deletion = 2083\n",
            "Total 143 corrupted image moved to 'corrupted_images' folder\n",
            "\n",
            "Splitting files in Train, Validation and Test and saving to data/4_tfds_dataset/\n",
            "Copying files: 1940 files [00:00, 1998.22 files/s]\n",
            "Oversampling: 7 classes [00:02,  2.35 classes/s]\n"
          ]
        }
      ],
      "source": [
        "!sh setup.sh\n",
        "!python preprocess.py --root data/1_extracted \\\n",
        "                      --oversample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LisTfP8OmsD9"
      },
      "outputs": [],
      "source": [
        "# import json\n",
        "# config = dict(\n",
        "#     notes=\"7 classes, keras-cv, augment:True, oversample, val 453\",\n",
        "#     root_dir=\"data/4_tfds_dataset\",\n",
        "#     project_name=\"rock-classification-with-keras-cv\",\n",
        "#     model_name=\"mobilenet\",\n",
        "#     num_classes=7,\n",
        "#     sample_size=1.0,\n",
        "#     augment=True,\n",
        "#     optimizer=\"adam\",\n",
        "#     init_learning_rate=0.01,\n",
        "#     batch_size=32,\n",
        "#     max_epochs=15,\n",
        "#     image_size=224,\n",
        "#     # lr_decay_rate = 0.7,\n",
        "#     loss_fn=\"categoricalcrossentropy\",\n",
        "#     metrics=['accuracy'],\n",
        "#     earlystopping_patience=10,\n",
        "#     lr_reduce_patience=5,\n",
        "# )\n",
        "\n",
        "# # save dictionary to config.json file\n",
        "# with open('config.json', 'w') as f:\n",
        "#     json.dump(config, f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "\"\"\"\n",
        "Trains a model on the dataset.\n",
        "\n",
        "Designed to show how to do a simple wandb integration with keras.\n",
        "\"\"\"\n",
        "\n",
        "from data_utilities import get_data_tfds, prepare_dataset\n",
        "from model_utilities import get_model, get_optimizer\n",
        "from augment_utilities import apply_rand_augment, cut_mix_and_mix_up, preprocess_for_model\n",
        "\n",
        "# from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow.random import set_seed\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras.backend import clear_session\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.data import AUTOTUNE\n",
        "\n",
        "from wandb.keras import WandbCallback\n",
        "import wandb\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# *IMPORANT*: Have to do this line *before* importing tensorflow\n",
        "# os.environ['PYTHONHASHSEED'] = str(1)\n",
        "\n",
        "\n",
        "def reset_random_seeds():\n",
        "    os.environ['PYTHONHASHSEED'] = str(1)\n",
        "    set_seed(1)\n",
        "    np.random.seed(1)\n",
        "    random.seed(1)\n",
        "\n",
        "\n",
        "def load_dataset(split=\"train\"):\n",
        "    dataset = data[split]\n",
        "    return prepare_dataset(dataset, split)\n",
        "\n",
        "\n",
        "# read config file\n",
        "with open('config.json') as config_file:\n",
        "    config = json.load(config_file)\n",
        "\n",
        "\n",
        "\n",
        "reset_random_seeds()\n",
        "\n",
        "run = wandb.init(config=config)\n",
        "\n",
        "config = wandb.config\n",
        "\n",
        "data, builder = get_data_tfds()\n",
        "\n",
        "num_classes = builder.info.features['label'].num_classes\n",
        "config[\"num_classes\"] = num_classes\n",
        "\n",
        "IMAGE_SIZE = (config[\"image_size\"], config[\"image_size\"])\n",
        "\n",
        "if config.augment:\n",
        "    train_dataset = (\n",
        "        load_dataset()\n",
        "        .map(apply_rand_augment, num_parallel_calls=AUTOTUNE)\n",
        "        .map(cut_mix_and_mix_up, num_parallel_calls=AUTOTUNE)\n",
        "    )\n",
        "else:\n",
        "    train_dataset = load_dataset()\n",
        "\n",
        "# visualize_dataset(train_dataset, \"CutMix, MixUp and RandAugment\")\n",
        "\n",
        "train_dataset = train_dataset.map(preprocess_for_model, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "val_dataset = load_dataset(split=\"val\")\n",
        "val_dataset = val_dataset.map(preprocess_for_model, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "# test_dataset = load_dataset(split=\"test\")\n",
        "# test_dataset = test_dataset.map(preprocess_for_model, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "labels = builder.info.features['label'].names\n",
        "\n",
        "# build model\n",
        "clear_session()\n",
        "model = get_model(config, config.num_classes)\n",
        "\n",
        "opt = get_optimizer(config)\n",
        "\n",
        "config.metrics.append(tfa.metrics.F1Score(\n",
        "    num_classes=config.num_classes,\n",
        "    average='macro',\n",
        "    threshold=0.5))\n",
        "\n",
        "# Notice that we use label_smoothing=0.1 in the loss function.\n",
        "# When using MixUp, label smoothing is highly recommended.\n",
        "\n",
        "model.compile(loss=losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
        "                optimizer=opt,\n",
        "                metrics=config[\"metrics\"])\n",
        "\n",
        "\n",
        "wandbcallback = WandbCallback()\n",
        "\n",
        "# Define WandbCallback for experiment tracking\n",
        "\n",
        "callbacks = [wandbcallback]\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=config[\"max_epochs\"],\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "run.finish()"
      ],
      "metadata": {
        "id": "2Wd0UGX4msKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "d3Abr4A4msHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vtS8Vw37msFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "p8k6mjLimsA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7nITexrumr-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RzoEBJQQmr3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKG91Awf_DVH"
      },
      "outputs": [],
      "source": [
        "# !python train.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa9mUak6P7bm"
      },
      "source": [
        "# Weights and Biases Sweeps"
      ]
    }
  ]
}